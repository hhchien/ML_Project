/media/data/huyennm/ML/training_wes_copy.py:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
  from distutils.command.config import config
wandb: Currently logged in as: mnhhuyen-26 (huyennm). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /media/data/huyennm/ML/wandb/run-20230614_205644-skhjnw3w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run WES_ws_16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/huyennm/Mutation_Detection
wandb: üöÄ View run at https://wandb.ai/huyennm/Mutation_Detection/runs/skhjnw3w
Training with WES datasets --- Window size = 16
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
219477 219477
(197529, 5, 33, 3) (197529, 3)
(21948, 5, 33, 3) (21948, 3)
(896, 5, 33, 3) (896, 3)
Balanced weights: [0.14904647 0.19026911 0.32735109]
Config: {'batch_size': 32, 'shuffle': True, 'num_workers': 3}
100
Training infor:
 - epoch: 100 
 - Leanring rate: 0.001 
 - Learning rate scheduler: True 
 - Early stopping: False 

[INFO] training the network...
/media/data/huyennm/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[epoch: 1 -- Lr: [0.001] -- time: 82.57401371002197s]  --- Acc_train: 0.713647110044601 --- loss_train : 0.09981347636862009 --- Acc_val: 0.746172772006561 --- loss_val : 0.09134296641057793 --- f1_score: 0.7480706213231121
[epoch: 2 -- Lr: [0.001] -- time: 50.062018394470215s]  --- Acc_train: 0.7512112145558374 --- loss_train : 0.09062395581825804 --- Acc_val: 0.7551485328959359 --- loss_val : 0.08871723714978359 --- f1_score: 0.7560612788645228
[epoch: 3 -- Lr: [0.001] -- time: 77.2958779335022s]  --- Acc_train: 0.7558991337980753 --- loss_train : 0.08921599688726774 --- Acc_val: 0.7596136322216147 --- loss_val : 0.0880084448114386 --- f1_score: 0.7605941732740201
[epoch: 4 -- Lr: [0.001] -- time: 48.66349744796753s]  --- Acc_train: 0.7594581048858648 --- loss_train : 0.08813349186095208 --- Acc_val: 0.7567887734645525 --- loss_val : 0.08768985639793932 --- f1_score: 0.7578917433189899
[epoch: 5 -- Lr: [0.001] -- time: 77.23614001274109s]  --- Acc_train: 0.7608249927858695 --- loss_train : 0.08754863524037032 --- Acc_val: 0.7516402405686168 --- loss_val : 0.08908724675021044 --- f1_score: 0.7523540278190085
[epoch: 6 -- Lr: [0.001] -- time: 53.07011604309082s]  --- Acc_train: 0.7618628150803173 --- loss_train : 0.08722658974176502 --- Acc_val: 0.7550574084199015 --- loss_val : 0.08751555876921939 --- f1_score: 0.7558664886693794
[epoch: 7 -- Lr: [0.001] -- time: 75.92775058746338s]  --- Acc_train: 0.7639435222169909 --- loss_train : 0.08674445061864755 --- Acc_val: 0.7600692546017861 --- loss_val : 0.08613740401281697 --- f1_score: 0.7615022009194434
[epoch: 8 -- Lr: [0.001] -- time: 51.24073529243469s]  --- Acc_train: 0.7640852735547692 --- loss_train : 0.08645089950858963 --- Acc_val: 0.7638053581191908 --- loss_val : 0.08577807080737124 --- f1_score: 0.7649234144898076
[epoch: 9 -- Lr: [0.001] -- time: 75.43196535110474s]  --- Acc_train: 0.7650623452758836 --- loss_train : 0.08624153591572387 --- Acc_val: 0.7715509385821031 --- loss_val : 0.08535355712310223 --- f1_score: 0.7720526068318478
[epoch: 10 -- Lr: [0.001] -- time: 47.59443163871765s]  --- Acc_train: 0.7662267312647764 --- loss_train : 0.085798451443376 --- Acc_val: 0.7634864224530709 --- loss_val : 0.08639428233593821 --- f1_score: 0.7643319849894162
[epoch: 11 -- Lr: [0.001] -- time: 75.21777033805847s]  --- Acc_train: 0.7673455543236689 --- loss_train : 0.08565319233720174 --- Acc_val: 0.7660379077820303 --- loss_val : 0.0859990533642262 --- f1_score: 0.7663317682102219
[epoch: 12 -- Lr: [0.001] -- time: 55.98810172080994s]  --- Acc_train: 0.7667127358514446 --- loss_train : 0.08557053329100724 --- Acc_val: 0.7677237105886641 --- loss_val : 0.08552502674974964 --- f1_score: 0.768787857183842
[epoch: 13 -- Lr: [0.001] -- time: 76.21796035766602s]  --- Acc_train: 0.7679733102481154 --- loss_train : 0.0853542665126335 --- Acc_val: 0.769455075633315 --- loss_val : 0.08563462162754881 --- f1_score: 0.7699449910601107
[epoch: 14 -- Lr: [0.001] -- time: 56.89300894737244s]  --- Acc_train: 0.7682163125414496 --- loss_train : 0.08517222900483486 --- Acc_val: 0.7654000364497904 --- loss_val : 0.08566435818941456 --- f1_score: 0.7668297196571912
[epoch: 15 -- Lr: [0.001] -- time: 75.28993558883667s]  --- Acc_train: 0.7686111912681176 --- loss_train : 0.0850297127914027 --- Acc_val: 0.7644432294514306 --- loss_val : 0.0851918723263725 --- f1_score: 0.7654563552530875
[epoch: 16 -- Lr: [0.001] -- time: 54.63051986694336s]  --- Acc_train: 0.7674873056614472 --- loss_train : 0.08501388704066393 --- Acc_val: 0.7647166028795335 --- loss_val : 0.08550857256744492 --- f1_score: 0.7660383491900699
[epoch: 17 -- Lr: [0.001] -- time: 75.95338702201843s]  --- Acc_train: 0.7689655696125632 --- loss_train : 0.08474203700504165 --- Acc_val: 0.7633497357390195 --- loss_val : 0.08557489964210015 --- f1_score: 0.764137704030124
[epoch: 18 -- Lr: [0.001] -- time: 56.144649267196655s]  --- Acc_train: 0.768894693943674 --- loss_train : 0.08482495247937931 --- Acc_val: 0.7662201567340988 --- loss_val : 0.08572290512491862 --- f1_score: 0.7671260092491226
[epoch: 19 -- Lr: [0.001] -- time: 75.28530144691467s]  --- Acc_train: 0.769390823625898 --- loss_train : 0.0845623534471757 --- Acc_val: 0.7635775469291052 --- loss_val : 0.08474799917713251 --- f1_score: 0.7646974481279949
[epoch: 20 -- Lr: [0.001] -- time: 46.48221778869629s]  --- Acc_train: 0.7698515154736773 --- loss_train : 0.08448811646467105 --- Acc_val: 0.7665390924002187 --- loss_val : 0.08584323610302061 --- f1_score: 0.7677314766293094
[epoch: 21 -- Lr: [0.001] -- time: 75.78998684883118s]  --- Acc_train: 0.7698717656647884 --- loss_train : 0.08439439509503165 --- Acc_val: 0.7687716420630581 --- loss_val : 0.08515360501667484 --- f1_score: 0.769531950371714
[epoch: 22 -- Lr: [0.001] -- time: 53.68694472312927s]  --- Acc_train: 0.7703375200603456 --- loss_train : 0.08427968616144807 --- Acc_val: 0.7714142518680518 --- loss_val : 0.08445679006616805 --- f1_score: 0.7721784013470784
[epoch: 23 -- Lr: [0.001] -- time: 78.46983790397644s]  --- Acc_train: 0.7701451432447893 --- loss_train : 0.0841530104809722 --- Acc_val: 0.7701385092035721 --- loss_val : 0.08460574654752648 --- f1_score: 0.7709826061168228
[epoch: 24 -- Lr: [0.001] -- time: 52.43430161476135s]  --- Acc_train: 0.770378020442568 --- loss_train : 0.08400272689787296 --- Acc_val: 0.7670402770184072 --- loss_val : 0.08554587711690016 --- f1_score: 0.7678892358425612
[epoch: 25 -- Lr: [0.001] -- time: 76.06193685531616s]  --- Acc_train: 0.7704235833725681 --- loss_train : 0.08385768134037311 --- Acc_val: 0.7708219427738291 --- loss_val : 0.08465121162320732 --- f1_score: 0.7717122235605619
[epoch: 26 -- Lr: [0.001] -- time: 54.5621452331543s]  --- Acc_train: 0.7717752836292393 --- loss_train : 0.08378663839321242 --- Acc_val: 0.7693183889192637 --- loss_val : 0.08402925215567963 --- f1_score: 0.7700743927734847
[epoch: 27 -- Lr: [0.001] -- time: 73.00472450256348s]  --- Acc_train: 0.7713145917814599 --- loss_train : 0.08365873516827539 --- Acc_val: 0.7723254966283943 --- loss_val : 0.08386147993613273 --- f1_score: 0.7729725829354192
[epoch: 28 -- Lr: [0.001] -- time: 41.05804777145386s]  --- Acc_train: 0.7721296619736848 --- loss_train : 0.08357534521787993 --- Acc_val: 0.7627118644067796 --- loss_val : 0.08516448610952991 --- f1_score: 0.7636478625111222
[epoch: 29 -- Lr: [0.001] -- time: 28.202882051467896s]  --- Acc_train: 0.7724283522925748 --- loss_train : 0.08344637211008313 --- Acc_val: 0.7692728266812466 --- loss_val : 0.08626813748872267 --- f1_score: 0.7698925127010351
[epoch: 30 -- Lr: [0.001] -- time: 27.96894407272339s]  --- Acc_train: 0.7719625978970176 --- loss_train : 0.08345731013838462 --- Acc_val: 0.7722799343903772 --- loss_val : 0.08405845513882086 --- f1_score: 0.773277383134275
[epoch: 31 -- Lr: [0.001] -- time: 30.67372465133667s]  --- Acc_train: 0.772792855732576 --- loss_train : 0.08329401040960423 --- Acc_val: 0.7680882084928011 --- loss_val : 0.08396724072557014 --- f1_score: 0.7686642538497463
[epoch: 32 -- Lr: [0.0009000000000000001] -- time: 27.301144123077393s]  --- Acc_train: 0.7727877931847982 --- loss_train : 0.0829999363452519 --- Acc_val: 0.7645343539274649 --- loss_val : 0.08414244786496346 --- f1_score: 0.765533798017351
[epoch: 33 -- Lr: [0.0008100000000000001] -- time: 27.878167152404785s]  --- Acc_train: 0.7746356231236933 --- loss_train : 0.08269476955436825 --- Acc_val: 0.773555677054857 --- loss_val : 0.08314980898672227 --- f1_score: 0.7739975107587168
[epoch: 34 -- Lr: [0.0007290000000000002] -- time: 27.206679105758667s]  --- Acc_train: 0.7752785666914731 --- loss_train : 0.08231918789568791 --- Acc_val: 0.7686349553490067 --- loss_val : 0.08347538055362384 --- f1_score: 0.7695476527653702
[epoch: 35 -- Lr: [0.0006561000000000001] -- time: 27.85465955734253s]  --- Acc_train: 0.775283629239251 --- loss_train : 0.08224466002238662 --- Acc_val: 0.7733278658647713 --- loss_val : 0.08284505393183665 --- f1_score: 0.7742180787380334
[epoch: 36 -- Lr: [0.00059049] -- time: 29.938462495803833s]  --- Acc_train: 0.7762353882214763 --- loss_train : 0.08176176751084153 --- Acc_val: 0.7720976854383087 --- loss_val : 0.08301447555211611 --- f1_score: 0.7731848213910325
[epoch: 37 -- Lr: [0.000531441] -- time: 27.861393451690674s]  --- Acc_train: 0.7762809511514764 --- loss_train : 0.0816939534883916 --- Acc_val: 0.7757882267176964 --- loss_val : 0.08252745350610197 --- f1_score: 0.7766894202865101
[epoch: 38 -- Lr: [0.0004782969000000001] -- time: 28.607372760772705s]  --- Acc_train: 0.777182084655924 --- loss_train : 0.08141261147592262 --- Acc_val: 0.7736923637689084 --- loss_val : 0.08303037589430101 --- f1_score: 0.7746001745364294
[epoch: 39 -- Lr: [0.0004304672100000001] -- time: 28.52186942100525s]  --- Acc_train: 0.7778655286059262 --- loss_train : 0.08122315176619563 --- Acc_val: 0.7761527246218334 --- loss_val : 0.08241405540379462 --- f1_score: 0.7770435661766345
[epoch: 40 -- Lr: [0.0003874204890000001] -- time: 27.616706132888794s]  --- Acc_train: 0.7783768459314835 --- loss_train : 0.0809302417911385 --- Acc_val: 0.7722343721523601 --- loss_val : 0.08244083828026168 --- f1_score: 0.773175323428836
[epoch: 41 -- Lr: [0.0003486784401000001] -- time: 27.439593076705933s]  --- Acc_train: 0.7783717833837057 --- loss_train : 0.0807102993070544 --- Acc_val: 0.7730544924366685 --- loss_val : 0.08287707821883274 --- f1_score: 0.7738999519552313
[epoch: 42 -- Lr: [0.0003138105960900001] -- time: 28.329729318618774s]  --- Acc_train: 0.778214844402594 --- loss_train : 0.08057218262738695 --- Acc_val: 0.7740568616730453 --- loss_val : 0.08230711551716564 --- f1_score: 0.7751056880337189
[epoch: 43 -- Lr: [0.0002824295364810001] -- time: 28.894384145736694s]  --- Acc_train: 0.7787210991803735 --- loss_train : 0.0804732124226832 --- Acc_val: 0.7756971022416621 --- loss_val : 0.08209989357451021 --- f1_score: 0.776533117023894
[epoch: 44 -- Lr: [0.0002541865828329001] -- time: 29.117748975753784s]  --- Acc_train: 0.7787717246581515 --- loss_train : 0.08034723242128958 --- Acc_val: 0.772188809914343 --- loss_val : 0.08267568896733436 --- f1_score: 0.7728954867943771
[epoch: 45 -- Lr: [0.0002287679245496101] -- time: 27.727592945098877s]  --- Acc_train: 0.7787666621103737 --- loss_train : 0.08019347014314399 --- Acc_val: 0.7750136686714051 --- loss_val : 0.08224488723414118 --- f1_score: 0.7758258340021221
[epoch: 46 -- Lr: [0.0002058911320946491] -- time: 27.263675212860107s]  --- Acc_train: 0.7795614821114875 --- loss_train : 0.08000790499869358 --- Acc_val: 0.7744669218151996 --- loss_val : 0.0822501280148026 --- f1_score: 0.7753419887632119
[epoch: 47 -- Lr: [0.00018530201888518417] -- time: 28.294797897338867s]  --- Acc_train: 0.7798854851692663 --- loss_train : 0.08005224335322439 --- Acc_val: 0.774968106433388 --- loss_val : 0.08246449795874329 --- f1_score: 0.7757969160805515
[epoch: 48 -- Lr: [0.00016677181699666576] -- time: 27.8961763381958s]  --- Acc_train: 0.779794359309266 --- loss_train : 0.0798220143918708 --- Acc_val: 0.7735101148168398 --- loss_val : 0.08222838471060274 --- f1_score: 0.7744121445600965
[epoch: 49 -- Lr: [0.00015009463529699917] -- time: 26.80202865600586s]  --- Acc_train: 0.7794703562514871 --- loss_train : 0.07989443949148241 --- Acc_val: 0.7754692910515765 --- loss_val : 0.08187668346001722 --- f1_score: 0.7763969769719534
[epoch: 50 -- Lr: [0.0001350851717672993] -- time: 27.93101978302002s]  --- Acc_train: 0.7805486789281574 --- loss_train : 0.0796086049549034 --- Acc_val: 0.7754237288135594 --- loss_val : 0.08202430156820795 --- f1_score: 0.7762812852674593
[epoch: 51 -- Lr: [0.00012157665459056935] -- time: 28.401272535324097s]  --- Acc_train: 0.7808878696292696 --- loss_train : 0.07961675683758222 --- Acc_val: 0.7757426644796792 --- loss_val : 0.08194860598739949 --- f1_score: 0.7766007548419578
[epoch: 52 -- Lr: [0.00010941898913151242] -- time: 27.48350715637207s]  --- Acc_train: 0.7801588627492672 --- loss_train : 0.07959569240263488 --- Acc_val: 0.7758793511937306 --- loss_val : 0.08176366672574917 --- f1_score: 0.7767651657527371
[epoch: 53 -- Lr: [9.847709021836118e-05] -- time: 26.81257677078247s]  --- Acc_train: 0.7802297384181563 --- loss_train : 0.07963342068412307 --- Acc_val: 0.7741024239110625 --- loss_val : 0.08192112002801596 --- f1_score: 0.7749484923485231
[epoch: 54 -- Lr: [8.862938119652506e-05] -- time: 28.20973801612854s]  --- Acc_train: 0.78033605192149 --- loss_train : 0.07941464631561174 --- Acc_val: 0.7743757973391653 --- loss_val : 0.08189296024514629 --- f1_score: 0.7752557403034399
[epoch: 55 -- Lr: [7.976644307687256e-05] -- time: 28.707741260528564s]  --- Acc_train: 0.7799107979081552 --- loss_train : 0.07937533696080308 --- Acc_val: 0.7746036085292509 --- loss_val : 0.0819115624497988 --- f1_score: 0.775459033383259
[epoch: 56 -- Lr: [7.17897987691853e-05] -- time: 27.884170293807983s]  --- Acc_train: 0.781151122113715 --- loss_train : 0.0793577772998183 --- Acc_val: 0.7764716602879533 --- loss_val : 0.08183857524074076 --- f1_score: 0.7772674674330202
[epoch: 57 -- Lr: [6.461081889226677e-05] -- time: 26.974331855773926s]  --- Acc_train: 0.7815156255537161 --- loss_train : 0.07926350755843241 --- Acc_val: 0.7751047931474394 --- loss_val : 0.08182715957182711 --- f1_score: 0.7759940052781695
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:  acc_train ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    acc_val ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   f1_score ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: loss_train ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss_val ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:  acc_train 0.78152
wandb:    acc_val 0.7751
wandb:   f1_score 0.77599
wandb: loss_train 0.07926
wandb:   loss_val 0.08183
wandb: 
wandb: üöÄ View run WES_ws_16 at: https://wandb.ai/huyennm/Mutation_Detection/runs/skhjnw3w
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230614_205644-skhjnw3w/logs
Stopping training in epoch 58
****** Testing model with best checkpoint******
[478. 237.  11.] [597. 283.  16.] [0.80067002 0.83745583 0.6875    ] 0.8102678571428571
              precision    recall  f1-score   support

           0       0.90      0.80      0.85       597
           1       0.68      0.84      0.75       283
           2       0.65      0.69      0.67        16

    accuracy                           0.81       896
   macro avg       0.74      0.78      0.75       896
weighted avg       0.83      0.81      0.81       896

****** Testing model with last checkpoint******
[477. 237.  11.] [597. 283.  16.] [0.79899497 0.83745583 0.6875    ] 0.8091517857142857
              precision    recall  f1-score   support

           0       0.90      0.80      0.85       597
           1       0.68      0.84      0.75       283
           2       0.65      0.69      0.67        16

    accuracy                           0.81       896
   macro avg       0.74      0.77      0.75       896
weighted avg       0.83      0.81      0.81       896

