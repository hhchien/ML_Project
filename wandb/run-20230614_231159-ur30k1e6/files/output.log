Training with WES datasets --- Window size = 10
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
229675 229675
(206707, 5, 21, 3) (206707, 3)
(22968, 5, 21, 3) (22968, 3)
(941, 5, 21, 3) (941, 3)
Balanced weights: [0.14772762 0.19151424 0.32742481]
Config: {'batch_size': 32, 'shuffle': True, 'num_workers': 3}
60
Training infor:
 - epoch: 100
 - Leanring rate: 0.001
 - Learning rate scheduler: True
 - Early stopping: False
[INFO] training the network...
[epoch: 1 -- Lr: [0.001] -- time: 122.1379976272583s]  --- Acc_train: 0.6814234641303875 --- loss_train : 0.10440274448738043 --- Acc_val: 0.7301898293277603 --- loss_val : 0.09287220073359416 --- f1_score: 0.7316734159561554
/media/data/huyennm/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[epoch: 2 -- Lr: [0.001] -- time: 28.83645534515381s]  --- Acc_train: 0.7381027251133246 --- loss_train : 0.09194882054870476 --- Acc_val: 0.7451671891327064 --- loss_val : 0.0908859823991687 --- f1_score: 0.7465272954350444
[epoch: 3 -- Lr: [0.001] -- time: 30.33888602256775s]  --- Acc_train: 0.7480491710488759 --- loss_train : 0.08983599037057367 --- Acc_val: 0.7538749564611633 --- loss_val : 0.08905579558503088 --- f1_score: 0.7548719445268725
[epoch: 4 -- Lr: [0.001] -- time: 29.72142004966736s]  --- Acc_train: 0.752906287643863 --- loss_train : 0.08858196480377388 --- Acc_val: 0.7567049808429118 --- loss_val : 0.08796231340525221 --- f1_score: 0.7579125642663999
[epoch: 5 -- Lr: [0.001] -- time: 29.589627981185913s]  --- Acc_train: 0.755489654438408 --- loss_train : 0.08782407625576266 --- Acc_val: 0.7576192964123999 --- loss_val : 0.0873903559351376 --- f1_score: 0.7587689107249248
[epoch: 6 -- Lr: [0.001] -- time: 29.412594318389893s]  --- Acc_train: 0.7571635213127761 --- loss_train : 0.08709402459115559 --- Acc_val: 0.7590125391849529 --- loss_val : 0.0871360010558689 --- f1_score: 0.7601507240576828
[epoch: 7 -- Lr: [0.001] -- time: 29.128035068511963s]  --- Acc_train: 0.7592340849608383 --- loss_train : 0.0867864076461537 --- Acc_val: 0.7629310344827587 --- loss_val : 0.08619207591691261 --- f1_score: 0.7638338456964303
[epoch: 8 -- Lr: [0.001] -- time: 28.599291801452637s]  --- Acc_train: 0.7604048242197894 --- loss_train : 0.0862250310015556 --- Acc_val: 0.7654127481713688 --- loss_val : 0.08598724067732257 --- f1_score: 0.766220311108156
[epoch: 9 -- Lr: [0.001] -- time: 29.978344440460205s]  --- Acc_train: 0.7610095449113963 --- loss_train : 0.08595673923932602 --- Acc_val: 0.7594043887147336 --- loss_val : 0.08676674696152166 --- f1_score: 0.7600268324635522
[epoch: 10 -- Lr: [0.001] -- time: 29.206165075302124s]  --- Acc_train: 0.7629688399522029 --- loss_train : 0.0855940641519425 --- Acc_val: 0.763758272378962 --- loss_val : 0.08577172123550521 --- f1_score: 0.7646142186088686
[epoch: 11 -- Lr: [0.001] -- time: 29.333188772201538s]  --- Acc_train: 0.7633365101326999 --- loss_train : 0.08529611159772951 --- Acc_val: 0.7631051898293277 --- loss_val : 0.0858608070400877 --- f1_score: 0.7642941627647721
[epoch: 12 -- Lr: [0.001] -- time: 30.161306619644165s]  --- Acc_train: 0.7643572786601325 --- loss_train : 0.08511651476613284 --- Acc_val: 0.7614942528735632 --- loss_val : 0.0850118818412056 --- f1_score: 0.7628119420096116
[epoch: 13 -- Lr: [0.001] -- time: 29.717037200927734s]  --- Acc_train: 0.7659440657549091 --- loss_train : 0.08484431505907984 --- Acc_val: 0.7646290491118077 --- loss_val : 0.08497744470165737 --- f1_score: 0.765764622213895
[epoch: 14 -- Lr: [0.001] -- time: 29.469517707824707s]  --- Acc_train: 0.7653151562356378 --- loss_train : 0.08479990863214822 --- Acc_val: 0.7641065830721003 --- loss_val : 0.08499198152417617 --- f1_score: 0.7650418651291796
[epoch: 15 -- Lr: [0.001] -- time: 30.78776788711548s]  --- Acc_train: 0.7660504965966319 --- loss_train : 0.08469039823193454 --- Acc_val: 0.7664576802507836 --- loss_val : 0.08425547225892033 --- f1_score: 0.7674462952409685
[epoch: 16 -- Lr: [0.001] -- time: 33.29362368583679s]  --- Acc_train: 0.7665391109154504 --- loss_train : 0.08436462293153087 --- Acc_val: 0.7638018112156043 --- loss_val : 0.08447314676289902 --- f1_score: 0.7650849293694885
[epoch: 17 -- Lr: [0.001] -- time: 30.716349363327026s]  --- Acc_train: 0.766621352929509 --- loss_train : 0.08437328975176892 --- Acc_val: 0.7632358063392546 --- loss_val : 0.08497140880455338 --- f1_score: 0.7639884680291392
[epoch: 18 -- Lr: [0.001] -- time: 29.17581844329834s]  --- Acc_train: 0.7665632997431147 --- loss_train : 0.08430716431451352 --- Acc_val: 0.7674590734935562 --- loss_val : 0.08450722432931475 --- f1_score: 0.7684037935773018
[epoch: 19 -- Lr: [0.001] -- time: 28.777358293533325s]  --- Acc_train: 0.7659440657549091 --- loss_train : 0.08415561835215225 --- Acc_val: 0.7638888888888888 --- loss_val : 0.08505378967781173 --- f1_score: 0.7650706609580756
[epoch: 20 -- Lr: [0.001] -- time: 29.623703718185425s]  --- Acc_train: 0.7666020018673775 --- loss_train : 0.08400872217192475 --- Acc_val: 0.766805990943922 --- loss_val : 0.08438490778109002 --- f1_score: 0.7681243908254921
[epoch: 21 -- Lr: [0.001] -- time: 29.610604524612427s]  --- Acc_train: 0.7673325044628387 --- loss_train : 0.0840394430925241 --- Acc_val: 0.7659352142110762 --- loss_val : 0.08403467740115224 --- f1_score: 0.7671834504933103
[epoch: 22 -- Lr: [0.001] -- time: 29.201308965682983s]  --- Acc_train: 0.7674824751943572 --- loss_train : 0.08387463079420265 --- Acc_val: 0.76619644723093 --- loss_val : 0.08515070865371886 --- f1_score: 0.7670024410466175
[epoch: 23 -- Lr: [0.001] -- time: 31.670978546142578s]  --- Acc_train: 0.7680484937617014 --- loss_train : 0.08378945118258092 --- Acc_val: 0.7617554858934169 --- loss_val : 0.0850553998146168 --- f1_score: 0.7623441223469006
[epoch: 24 -- Lr: [0.001] -- time: 29.189887285232544s]  --- Acc_train: 0.7687257809363012 --- loss_train : 0.08372250860268919 --- Acc_val: 0.7676332288401254 --- loss_val : 0.08426106887676872 --- f1_score: 0.768308269394431
[epoch: 25 -- Lr: [0.001] -- time: 28.314401865005493s]  --- Acc_train: 0.7686193500945783 --- loss_train : 0.083563237608346 --- Acc_val: 0.7627133402995472 --- loss_val : 0.08452977545369418 --- f1_score: 0.7638102077641671
[epoch: 26 -- Lr: [0.001] -- time: 30.24927020072937s]  --- Acc_train: 0.7684403527698627 --- loss_train : 0.08359018526732523 --- Acc_val: 0.7661529083942877 --- loss_val : 0.08489745063420032 --- f1_score: 0.7669000031479057
[epoch: 27 -- Lr: [0.001] -- time: 29.573122262954712s]  --- Acc_train: 0.7687596452950312 --- loss_train : 0.083507727896255 --- Acc_val: 0.765282131661442 --- loss_val : 0.08619646467446956 --- f1_score: 0.7648905812045184
[epoch: 28 -- Lr: [0.001] -- time: 30.00284743309021s]  --- Acc_train: 0.7692676106759809 --- loss_train : 0.08346232501754094 --- Acc_val: 0.7674155346569139 --- loss_val : 0.08397135590064911 --- f1_score: 0.7679807529004288
[epoch: 29 -- Lr: [0.001] -- time: 29.57329559326172s]  --- Acc_train: 0.7698384670088579 --- loss_train : 0.08328333242337092 --- Acc_val: 0.7563131313131313 --- loss_val : 0.0855619864304583 --- f1_score: 0.7578289389416374
[epoch: 30 -- Lr: [0.001] -- time: 29.522152423858643s]  --- Acc_train: 0.7696497941530765 --- loss_train : 0.08323719477758391 --- Acc_val: 0.7695924764890282 --- loss_val : 0.08401797661805038 --- f1_score: 0.770232482128311
[epoch: 31 -- Lr: [0.001] -- time: 29.717429161071777s]  --- Acc_train: 0.7697610627603323 --- loss_train : 0.08323675582524795 --- Acc_val: 0.7662835249042146 --- loss_val : 0.08396495636929222 --- f1_score: 0.7674833753831256
[epoch: 32 -- Lr: [0.0009000000000000001] -- time: 30.223201513290405s]  --- Acc_train: 0.7698723313675879 --- loss_train : 0.08287978685927018 --- Acc_val: 0.7689829327760362 --- loss_val : 0.08386000488474961 --- f1_score: 0.7700483202298265
[epoch: 33 -- Lr: [0.0008100000000000001] -- time: 30.277945041656494s]  --- Acc_train: 0.7705979961975163 --- loss_train : 0.0826116644699112 --- Acc_val: 0.76619644723093 --- loss_val : 0.08401764009050923 --- f1_score: 0.7671790254405237
[epoch: 34 -- Lr: [0.0007290000000000002] -- time: 30.762499570846558s]  --- Acc_train: 0.7717638976909346 --- loss_train : 0.08227181993294369 --- Acc_val: 0.7616684082201324 --- loss_val : 0.08446683245218571 --- f1_score: 0.7630406946430439
[epoch: 35 -- Lr: [0.0006561000000000001] -- time: 29.369425773620605s]  --- Acc_train: 0.7726830731421771 --- loss_train : 0.0821641254711588 --- Acc_val: 0.7677638453500523 --- loss_val : 0.08321110858571316 --- f1_score: 0.7688271856541118
[epoch: 36 -- Lr: [0.00059049] -- time: 29.92584466934204s]  --- Acc_train: 0.7728233683426299 --- loss_train : 0.08189862474593473 --- Acc_val: 0.7700714036920934 --- loss_val : 0.08309871912735778 --- f1_score: 0.7711423534090914
[epoch: 37 -- Lr: [0.000531441] -- time: 30.396297693252563s]  --- Acc_train: 0.7739554054773181 --- loss_train : 0.08173195053029318 --- Acc_val: 0.7688958551027516 --- loss_val : 0.08284698453199131 --- f1_score: 0.7699668307268198
[epoch: 38 -- Lr: [0.0004782969000000001] -- time: 29.087223768234253s]  --- Acc_train: 0.7742408336437566 --- loss_train : 0.08157840710197127 --- Acc_val: 0.7644548937652386 --- loss_val : 0.08360354157043469 --- f1_score: 0.7659085618279433
[epoch: 39 -- Lr: [0.0004304672100000001] -- time: 29.515808820724487s]  --- Acc_train: 0.774531099575728 --- loss_train : 0.08122683360626587 --- Acc_val: 0.7676767676767676 --- loss_val : 0.08301789814376045 --- f1_score: 0.7687742687093847
[epoch: 40 -- Lr: [0.0003874204890000001] -- time: 28.789201259613037s]  --- Acc_train: 0.7749084452872907 --- loss_train : 0.08109193424273697 --- Acc_val: 0.7688087774294671 --- loss_val : 0.08258588916357439 --- f1_score: 0.7698707507502731
[epoch: 41 -- Lr: [0.0003486784401000001] -- time: 29.102867126464844s]  --- Acc_train: 0.7745939905276551 --- loss_train : 0.08102109052065347 --- Acc_val: 0.7660658307210031 --- loss_val : 0.08308729505874335 --- f1_score: 0.7673962836957383
[epoch: 42 -- Lr: [0.0003138105960900001] -- time: 28.979748964309692s]  --- Acc_train: 0.7755276792754963 --- loss_train : 0.08094163327107645 --- Acc_val: 0.7684604667363288 --- loss_val : 0.0825967745388264 --- f1_score: 0.7696508302766037
[epoch: 43 -- Lr: [0.0002824295364810001] -- time: 29.274463891983032s]  --- Acc_train: 0.776069509015176 --- loss_train : 0.08074453826103825 --- Acc_val: 0.7669801462904912 --- loss_val : 0.08273704474512875 --- f1_score: 0.7681877215140301
[epoch: 44 -- Lr: [0.0002541865828329001] -- time: 29.226886749267578s]  --- Acc_train: 0.7770709264804772 --- loss_train : 0.08047730373484539 --- Acc_val: 0.7711598746081505 --- loss_val : 0.08221079124213888 --- f1_score: 0.7721825594819114
[epoch: 45 -- Lr: [0.0002287679245496101] -- time: 29.4478178024292s]  --- Acc_train: 0.7758566473317304 --- loss_train : 0.08044931752195192 --- Acc_val: 0.7691135492859631 --- loss_val : 0.08260460115558826 --- f1_score: 0.7704073516747434
[epoch: 46 -- Lr: [0.0002058911320946491] -- time: 29.186301708221436s]  --- Acc_train: 0.7778159423725369 --- loss_train : 0.08027066441656594 --- Acc_val: 0.7698537095088819 --- loss_val : 0.0823166656661348 --- f1_score: 0.7709248334611373
[epoch: 47 -- Lr: [0.00018530201888518417] -- time: 30.058911323547363s]  --- Acc_train: 0.7771338174324043 --- loss_train : 0.08029395013589061 --- Acc_val: 0.7714646464646465 --- loss_val : 0.08236428475291332 --- f1_score: 0.7723937970023559
[epoch: 48 -- Lr: [0.00016677181699666576] -- time: 27.812414407730103s]  --- Acc_train: 0.7773418413503171 --- loss_train : 0.08027473385217973 --- Acc_val: 0.7698972483455242 --- loss_val : 0.08225283505431288 --- f1_score: 0.7710083021608328
[epoch: 49 -- Lr: [0.00015009463529699917] -- time: 28.99142074584961s]  --- Acc_train: 0.7769886844664187 --- loss_train : 0.08005385926805732 --- Acc_val: 0.7688087774294671 --- loss_val : 0.08229247165568844 --- f1_score: 0.769942901921466
Stopping training in epoch 50