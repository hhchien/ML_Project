Training with WES datasets --- Window size = 16
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
219477 219477
(197529, 5, 33, 3) (197529, 3)
(21948, 5, 33, 3) (21948, 3)
(896, 5, 33, 3) (896, 3)
Balanced weights: [0.14904647 0.19026911 0.32735109]
Config: {'batch_size': 32, 'shuffle': True, 'num_workers': 3}
100
Training infor:
 - epoch: 100
 - Leanring rate: 0.001
 - Learning rate scheduler: True
 - Early stopping: False
[INFO] training the network...
/media/data/huyennm/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[epoch: 1 -- Lr: [0.001] -- time: 70.92333650588989s]  --- Acc_train: 0.6966774498934334 --- loss_train : 0.10252362560868794 --- Acc_val: 0.7448514671040641 --- loss_val : 0.09318232466724462 --- f1_score: 0.7455799192771072
[epoch: 2 -- Lr: [0.001] -- time: 45.07888674736023s]  --- Acc_train: 0.7426504462635866 --- loss_train : 0.09187931545538512 --- Acc_val: 0.7471295790049207 --- loss_val : 0.0894672856057946 --- f1_score: 0.7475880973345225
[epoch: 3 -- Lr: [0.001] -- time: 60.0040922164917s]  --- Acc_train: 0.753241296214733 --- loss_train : 0.08949131031190206 --- Acc_val: 0.7374248223072717 --- loss_val : 0.08891084653946346 --- f1_score: 0.7369534202840923
[epoch: 4 -- Lr: [0.001] -- time: 65.60573244094849s]  --- Acc_train: 0.7580102162214156 --- loss_train : 0.08814193048327995 --- Acc_val: 0.762848551120831 --- loss_val : 0.08659947074012783 --- f1_score: 0.7635546215490983
[epoch: 5 -- Lr: [0.001] -- time: 36.53562140464783s]  --- Acc_train: 0.7603592383903123 --- loss_train : 0.08735767991098112 --- Acc_val: 0.7599325678877347 --- loss_val : 0.08764799079903977 --- f1_score: 0.7607139665713158
[epoch: 6 -- Lr: [0.001] -- time: 66.22509050369263s]  --- Acc_train: 0.7644497769947703 --- loss_train : 0.08664466945147024 --- Acc_val: 0.7685893931109896 --- loss_val : 0.08627532004333426 --- f1_score: 0.7692469823357632
[epoch: 7 -- Lr: [0.001] -- time: 60.68376111984253s]  --- Acc_train: 0.7654521614547737 --- loss_train : 0.08616250939881474 --- Acc_val: 0.7623473665026426 --- loss_val : 0.0854657477713039 --- f1_score: 0.7631320316837223
[epoch: 8 -- Lr: [0.001] -- time: 49.27572965621948s]  --- Acc_train: 0.7672038029858906 --- loss_train : 0.0854773344674541 --- Acc_val: 0.7685438308729725 --- loss_val : 0.08450799417365193 --- f1_score: 0.769623080617221
[epoch: 9 -- Lr: [0.001] -- time: 58.72096300125122s]  --- Acc_train: 0.7675379311392251 --- loss_train : 0.0852852560760485 --- Acc_val: 0.7739657371970111 --- loss_val : 0.085048980799446 --- f1_score: 0.7747213720592248
[epoch: 10 -- Lr: [0.001] -- time: 56.012162923812866s]  --- Acc_train: 0.7688136931792294 --- loss_train : 0.08486686881010767 --- Acc_val: 0.7648077273555677 --- loss_val : 0.08633971497383447 --- f1_score: 0.765464791857105
[epoch: 11 -- Lr: [0.001] -- time: 52.87106537818909s]  --- Acc_train: 0.7694920745814539 --- loss_train : 0.0845461774907476 --- Acc_val: 0.7723710588664116 --- loss_val : 0.08528416961962744 --- f1_score: 0.7728087279727941
[epoch: 12 -- Lr: [0.001] -- time: 237.62289118766785s]  --- Acc_train: 0.769588262989232 --- loss_train : 0.08436968411481828 --- Acc_val: 0.774968106433388 --- loss_val : 0.0841196238061989 --- f1_score: 0.7759686470078911
[epoch: 13 -- Lr: [0.001] -- time: 52.22642922401428s]  --- Acc_train: 0.7716841577692389 --- loss_train : 0.08404151262251794 --- Acc_val: 0.7761982868598506 --- loss_val : 0.0841868393723406 --- f1_score: 0.7767034163589915
[epoch: 14 -- Lr: [0.001] -- time: 52.67524337768555s]  --- Acc_train: 0.7712690288514598 --- loss_train : 0.08369026062893747 --- Acc_val: 0.7536449790413705 --- loss_val : 0.08539365161075713 --- f1_score: 0.7535503924192315
[epoch: 15 -- Lr: [0.001] -- time: 60.557161808013916s]  --- Acc_train: 0.7725903538214642 --- loss_train : 0.08342755878192631 --- Acc_val: 0.773601239292874 --- loss_val : 0.08350334909225994 --- f1_score: 0.7744855776551274
[epoch: 16 -- Lr: [0.001] -- time: 53.83068132400513s]  --- Acc_train: 0.77299029509591 --- loss_train : 0.08321349229292015 --- Acc_val: 0.7774284672863131 --- loss_val : 0.08329434277065118 --- f1_score: 0.777977029222835
[epoch: 17 -- Lr: [0.001] -- time: 54.06569314002991s]  --- Acc_train: 0.7734864247781339 --- loss_train : 0.0830710094731184 --- Acc_val: 0.7693639511572808 --- loss_val : 0.08256930965473805 --- f1_score: 0.7704124202207127
[epoch: 18 -- Lr: [0.001] -- time: 48.37015771865845s]  --- Acc_train: 0.7742356818492474 --- loss_train : 0.08286092255828031 --- Acc_val: 0.779478767997084 --- loss_val : 0.08425407252306807 --- f1_score: 0.779140878200952
[epoch: 19 -- Lr: [0.001] -- time: 44.62873315811157s]  --- Acc_train: 0.7750254393025834 --- loss_train : 0.08271593770766832 --- Acc_val: 0.7684982686349553 --- loss_val : 0.08337233055034066 --- f1_score: 0.7694316305235217
[epoch: 20 -- Lr: [0.001] -- time: 55.54601550102234s]  --- Acc_train: 0.7743622455436924 --- loss_train : 0.08249871344626056 --- Acc_val: 0.7806178239475123 --- loss_val : 0.08178269314960421 --- f1_score: 0.7812615587837765
[epoch: 21 -- Lr: [0.001] -- time: 55.94157910346985s]  --- Acc_train: 0.775850634590364 --- loss_train : 0.08238885271935897 --- Acc_val: 0.7746947330052852 --- loss_val : 0.08262479840730931 --- f1_score: 0.7754575989124387
[epoch: 22 -- Lr: [0.001] -- time: 44.781692028045654s]  --- Acc_train: 0.7752481914048064 --- loss_train : 0.08235020432893901 --- Acc_val: 0.7750136686714051 --- loss_val : 0.0833183828519439 --- f1_score: 0.7758335104210783
[epoch: 23 -- Lr: [0.001] -- time: 62.433592557907104s]  --- Acc_train: 0.7758658222336974 --- loss_train : 0.08223425686866459 --- Acc_val: 0.7787497721888099 --- loss_val : 0.08227943433381787 --- f1_score: 0.7794477211411958
[epoch: 24 -- Lr: [0.001] -- time: 53.312705516815186s]  --- Acc_train: 0.775946822998142 --- loss_train : 0.0819484474989816 --- Acc_val: 0.7776107162383816 --- loss_val : 0.08181838342178202 --- f1_score: 0.7783609188918876
[epoch: 25 -- Lr: [0.001] -- time: 45.55402588844299s]  --- Acc_train: 0.7763973897503658 --- loss_train : 0.08202377490772186 --- Acc_val: 0.7779752141425187 --- loss_val : 0.0822813146905498 --- f1_score: 0.7786741184998563
[epoch: 26 -- Lr: [0.001] -- time: 56.71361947059631s]  --- Acc_train: 0.7770302082225901 --- loss_train : 0.08168837992457373 --- Acc_val: 0.7678603973027155 --- loss_val : 0.08886830435731825 --- f1_score: 0.7655151660057445
[epoch: 27 -- Lr: [0.001] -- time: 54.09871029853821s]  --- Acc_train: 0.7766454545914777 --- loss_train : 0.08171819990846846 --- Acc_val: 0.7719609987242574 --- loss_val : 0.08233084824220248 --- f1_score: 0.7727167000084868
[epoch: 28 -- Lr: [0.001] -- time: 47.23167300224304s]  --- Acc_train: 0.7772934607070354 --- loss_train : 0.08165696956836604 --- Acc_val: 0.7691817022052123 --- loss_val : 0.08350798359644533 --- f1_score: 0.7700756372595392
[epoch: 29 -- Lr: [0.001] -- time: 57.76763200759888s]  --- Acc_train: 0.7774655873314804 --- loss_train : 0.08151656727724166 --- Acc_val: 0.7779296519045016 --- loss_val : 0.08247059739156397 --- f1_score: 0.7785670889830603
[epoch: 30 -- Lr: [0.001] -- time: 55.15991997718811s]  --- Acc_train: 0.7772276475859241 --- loss_train : 0.08139921064068238 --- Acc_val: 0.7774284672863131 --- loss_val : 0.08179434961195942 --- f1_score: 0.7783894330796365
[epoch: 31 -- Lr: [0.001] -- time: 50.65494513511658s]  --- Acc_train: 0.7783819084792613 --- loss_train : 0.08126044479071132 --- Acc_val: 0.7699562602515035 --- loss_val : 0.08245495892929618 --- f1_score: 0.7708248326888069
[epoch: 32 -- Lr: [0.0009000000000000001] -- time: 58.519550800323486s]  --- Acc_train: 0.7783717833837057 --- loss_train : 0.08101010828170815 --- Acc_val: 0.7777018407144158 --- loss_val : 0.08328068266463053 --- f1_score: 0.7774855087065534
[epoch: 33 -- Lr: [0.0008100000000000001] -- time: 49.97377061843872s]  --- Acc_train: 0.7791868535759306 --- loss_train : 0.08066180810302888 --- Acc_val: 0.7758337889557135 --- loss_val : 0.08151334835298205 --- f1_score: 0.7766540845837964
[epoch: 34 -- Lr: [0.0007290000000000002] -- time: 50.54264211654663s]  --- Acc_train: 0.7791564782892638 --- loss_train : 0.08033624971280513 --- Acc_val: 0.7797521414251868 --- loss_val : 0.08229021867666733 --- f1_score: 0.7803886203366475
[epoch: 35 -- Lr: [0.0006561000000000001] -- time: 60.98689317703247s]  --- Acc_train: 0.7801487376537116 --- loss_train : 0.08015004545333677 --- Acc_val: 0.7667213413522872 --- loss_val : 0.08265907101492337 --- f1_score: 0.7673161357039917
[epoch: 36 -- Lr: [0.00059049] -- time: 46.00891971588135s]  --- Acc_train: 0.7809992456803811 --- loss_train : 0.07981157416347119 --- Acc_val: 0.7744669218151996 --- loss_val : 0.08129115165106943 --- f1_score: 0.7756548177455165
[epoch: 37 -- Lr: [0.000531441] -- time: 54.998913288116455s]  --- Acc_train: 0.7813890618592713 --- loss_train : 0.07955973317967312 --- Acc_val: 0.7803444505194095 --- loss_val : 0.08246917461313828 --- f1_score: 0.7807331041830661
[epoch: 38 -- Lr: [0.0004782969000000001] -- time: 60.68618607521057s]  --- Acc_train: 0.7815915637703831 --- loss_train : 0.07944253646121399 --- Acc_val: 0.7799799526152724 --- loss_val : 0.08048850729947772 --- f1_score: 0.7808285807125717
[epoch: 39 -- Lr: [0.0004304672100000001] -- time: 42.345959186553955s]  --- Acc_train: 0.7824116965103858 --- loss_train : 0.07915805193916757 --- Acc_val: 0.7803900127574267 --- loss_val : 0.0831239811363699 --- f1_score: 0.7800696569489768
[epoch: 40 -- Lr: [0.0003874204890000001] -- time: 58.29058241844177s]  --- Acc_train: 0.783368518040389 --- loss_train : 0.07882060674714632 --- Acc_val: 0.7852196099872426 --- loss_val : 0.08066254535429097 --- f1_score: 0.7856940296078263
[epoch: 41 -- Lr: [0.0003486784401000001] -- time: 61.336483001708984s]  --- Acc_train: 0.7840671496337247 --- loss_train : 0.07871037457174651 --- Acc_val: 0.7738746127209769 --- loss_val : 0.08125106375564793 --- f1_score: 0.774817560857937
[epoch: 42 -- Lr: [0.0003138105960900001] -- time: 42.68135118484497s]  --- Acc_train: 0.7829027636448319 --- loss_train : 0.0786292172808489 --- Acc_val: 0.7826681246582832 --- loss_val : 0.08128985743496561 --- f1_score: 0.7833198294316192
[epoch: 43 -- Lr: [0.0002824295364810001] -- time: 61.24049711227417s]  --- Acc_train: 0.7841532129459472 --- loss_train : 0.0783595040954388 --- Acc_val: 0.7816657554219063 --- loss_val : 0.08147098157741497 --- f1_score: 0.7821766764864666
[epoch: 44 -- Lr: [0.0002541865828329001] -- time: 58.45255470275879s]  --- Acc_train: 0.7851758475970617 --- loss_train : 0.07832580339448517 --- Acc_val: 0.781210133041735 --- loss_val : 0.08274977544769309 --- f1_score: 0.7809597147844536
[epoch: 45 -- Lr: [0.0002287679245496101] -- time: 48.34345030784607s]  --- Acc_train: 0.7851606599537283 --- loss_train : 0.07809645064789729 --- Acc_val: 0.7810734463276836 --- loss_val : 0.08023208142157383 --- f1_score: 0.7820281783895068
[epoch: 46 -- Lr: [0.0002058911320946491] -- time: 68.5719165802002s]  --- Acc_train: 0.7845835295070597 --- loss_train : 0.07812405498989294 --- Acc_val: 0.7827136868963004 --- loss_val : 0.08128911880696442 --- f1_score: 0.7832058454246349
[epoch: 47 -- Lr: [0.00018530201888518417] -- time: 45.93223786354065s]  --- Acc_train: 0.7855049132026184 --- loss_train : 0.07804307489358214 --- Acc_val: 0.7813923819938036 --- loss_val : 0.0826767756117659 --- f1_score: 0.7807943642942511
[epoch: 48 -- Lr: [0.00016677181699666576] -- time: 67.5896954536438s]  --- Acc_train: 0.7860516683626202 --- loss_train : 0.07788615417235456 --- Acc_val: 0.7847184253690541 --- loss_val : 0.08069762315824461 --- f1_score: 0.7852537878065029
[epoch: 49 -- Lr: [0.00015009463529699917] -- time: 58.71267771720886s]  --- Acc_train: 0.7862389826303986 --- loss_train : 0.0778318000739882 --- Acc_val: 0.7835338071806087 --- loss_val : 0.08055402969591109 --- f1_score: 0.7841693526232723
[epoch: 50 -- Lr: [0.0001350851717672993] -- time: 53.025142431259155s]  --- Acc_train: 0.7859504174070643 --- loss_train : 0.07782303633009621 --- Acc_val: 0.782258064516129 --- loss_val : 0.08037584729423924 --- f1_score: 0.7832000497988604
[epoch: 51 -- Lr: [0.00012157665459056935] -- time: 72.83975577354431s]  --- Acc_train: 0.7859200421203976 --- loss_train : 0.07774142954241615 --- Acc_val: 0.7841716785128485 --- loss_val : 0.08016510817172583 --- f1_score: 0.7849497207218656
[epoch: 52 -- Lr: [0.00010941898913151242] -- time: 54.07038426399231s]  --- Acc_train: 0.7856719772792856 --- loss_train : 0.0777632074072839 --- Acc_val: 0.7838071806087115 --- loss_val : 0.08035543638908964 --- f1_score: 0.7846078775749035
Stopping training in epoch 53